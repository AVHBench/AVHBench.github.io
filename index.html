<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AVHBENCH: A CROSS-MODAL HALLUCINATION BENCHMARK FOR AUDIO-VISUAL LARGE LANGUAGE MODELS.">
  <meta name="keywords" content="Audio-visual learning, LLM, Hallucination">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models</title>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->
<!--    </div>-->

<!--  </div>-->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">

        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models</h1>


          <div class="is-size-4 publication-authors">
            <span class="author-block"><b>ICLR 2025</b></span>
          </div>


          <div class="is-size-7 publication-authors">
            <span class="author-block"></span>
          </div>



          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/view/kimsungbin">Kim Sung-Bin*</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hyunbin70.github.io/">Oh Hyun-Bin*</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="https://ami.postech.ac.kr/d81b1c7c-7c84-4904-808e-097513816ae1/">JungMok Lee</a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://ardasnck.github.io/">Arda Senocak</a><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://mm.kaist.ac.kr/joon/">Joon Son Chung</a><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://ami.postech.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>POSTECH,&nbsp</span>
            <span class="author-block"><sup>2</sup>KAIST&nbsp</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                                <a href="https://openreview.net/pdf?id=jTEKTdI3K9"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.18325" class="external-link button is-normal is-rounded is-dark">
<!--                <a class="external-link button is-normal is-rounded is-dark">-->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=zYnUyPHPEn8&t=3s"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
<!--                <a href="https://github.com/google/nerfies"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
                <a href="https://github.com/postech-ami/AVHBench"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                  </a>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
<div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 preload
                 playsinline
                 width="50%">
            <source src="./static/video/v3.mp4"
                    type="video/mp4">
          </video>
        </div>
</section> -->

<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <img src="./static/images/avhbench_teaser.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 100%;"
      />

      <h2 class="subtitle has-text-centered">
        We introduce <span class="dnerf">AVHBench</span>, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs.
      </h2>
    </div>
  </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Following the success of Large Language Models (LLMs), expanding their boundaries to new modalities represents a significant paradigm shift in multimodal understanding. 
            Human perception is inherently multimodal, relying not only on text but also on auditory and visual cues for a complete understanding of the world.
            In recognition of this fact, audio-visual LLMs have recently emerged. 
            Despite promising developments, the lack of dedicated benchmarks poses challenges for understanding and evaluating models. 
            In this work, we show that audio-visual LLMs struggle to discern subtle relationships between audio and visual signals, leading to hallucinations, underscoring the need for reliable benchmarks. 
            To address this, we introduce AVHBench, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs.
            Our benchmark includes tests for assessing hallucinations, as well as the crossmodal matching and reasoning abilities of these models. 
            Our results reveal that most existing audio-visual LLMs struggle with hallucinations caused by crossinteractions between modalities, due to their limited capacity to perceive complex multimodal signals and their relationships. 
            Additionally, we demonstrate that simple training with our AVHBench improves robustness of audio-visual LLMs against hallucinations.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Need to add an presentation video this part!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Video</h2>-->
<!--        <div class="publication-video">-->
<!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
<!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">Dataset Construction Pipeline</h2>
          <div class="column is-full-width has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/avhbench_pipeline.png" class="interpolation-image" alt="Interpolate start reference image."/>
            </figure>
            <br><br>
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <div class="content has-text-justified" style="max-width: 80%; margin: 0 auto;">
                  <p>
                    We devise a dataset construction pipeline with automated procedures, consisting of two main stages: Stage 1 involves
                    disentangling audio-visual information, and Stage 2 focuses on Question-and-Answer (QnA) generation for four different tasks. 
                    At the end of each stage, we verify and correct the automatically generated outputs by employing a minimal number of human annotators.
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Updated Section Title -->
      <div class="columns is-centered has-text-centered">
        <h3 class="title is-4">Qualitative Results on AVHBench</h3>
      </div>
      
      <!-- Figures Container -->
      <div class="container">
        
        <!-- Figure 1 -->
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/avhbench_qual1.png" alt="Example 1">
            </figure>
            <p><strong>Example 1</strong></p>
          </div>
        </div>
        
        <!-- Figure 2 -->
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/avhbench_qual2.png" alt="Example 2">
            </figure>
            <p><strong>Example 2</strong></p>
          </div>
        </div>
        
        <!-- Figure 3 -->
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/avhbench_qual3.png" alt="Example 3">
            </figure>
            <p><strong>Example 3</strong></p>
          </div>
        </div>
        
      </div>
    </div>
  </section>
  




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceeding{sung2024avhbench,
  title={AVHBench: A Cross-Modal Hallucination Evluation for Audio-Visual Large Language Models},
  author={Sung-Bin, Kim and Hyun-Bin, Oh and Lee, JungMok and Senocak, Arda and Chung, Joon Son and Oh, Tae-Hyun},
  booktitle={arXiv preprint arXiv:2410.18325},
  year={2024}
}</code></pre>
  </div>
</section>



<!-- <section class="section" id="Acknowledgment">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgment</h2>
    <p>
      This work was supported by IITP grant funded by Korea government (MSIT) (No.2021-0-02068, Artificial Intelligence Innovation Hub; No.RS-2023-00225630, Development of Artificial Intelligence for Text-based 3D Movie Generation; No.2022-0-00290, Visual Intelligence for Space-Time Understanding and Generation
based on Multi-layered Visual Common Sense; No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities).
    </p>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
<!--      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
            <a class="icon-link" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
              Source code mainly borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>